{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mansfire/Zebra/blob/main/Zebra_Forca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKii2o6g2Vgu",
        "outputId": "206664cb-d2aa-4767-d0c5-28bc47dfcf28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFGSgWuI2YtP",
        "outputId": "28eeae65-5f51-43ef-a4e1-fae82fa95052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch disease ID 1\n",
            "Failed to fetch disease ID 2\n",
            "Failed to fetch disease ID 3\n",
            "Failed to fetch disease ID 4\n",
            "Failed to fetch disease ID 12\n",
            "Failed to fetch disease ID 21\n",
            "Failed to fetch disease ID 66\n",
            "Failed to fetch disease ID 75\n",
            "Failed to fetch disease ID 89\n",
            "Failed to fetch disease ID 121\n",
            "Failed to fetch disease ID 149\n",
            "Failed to fetch disease ID 152\n",
            "Failed to fetch disease ID 153\n",
            "Failed to fetch disease ID 161\n",
            "Failed to fetch disease ID 196\n",
            "Failed to fetch disease ID 197\n",
            "Failed to fetch disease ID 203\n",
            "Failed to fetch disease ID 208\n",
            "Failed to fetch disease ID 228\n",
            "Failed to fetch disease ID 259\n",
            "Failed to fetch disease ID 260\n",
            "Failed to fetch disease ID 271\n",
            "Failed to fetch disease ID 299\n",
            "Failed to fetch disease ID 311\n",
            "Failed to fetch disease ID 339\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Base URLs\n",
        "orphanet_base_url = \"https://www.orpha.net/en/disease/detail/\"\n",
        "wikipedia_base_url = \"https://en.wikipedia.org/wiki/\"\n",
        "\n",
        "# Example disease IDs (Replace this with dynamic fetching)\n",
        "#disease_ids = range(1, 3475)  # Adjust range as needed\n",
        "disease_ids = range(1, 75)\n",
        "disease_data = []\n",
        "\n",
        "def get_wikipedia_summary(disease_name):\n",
        "    \"\"\"Fetch Wikipedia summary for a disease\"\"\"\n",
        "    search_url = wikipedia_base_url + disease_name.replace(\" \", \"_\")\n",
        "    response = requests.get(search_url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        wiki_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Extract the first paragraph of the Wikipedia page\n",
        "        p_tags = wiki_soup.find_all(\"p\")\n",
        "        for p in p_tags:\n",
        "            text = p.text.strip()\n",
        "            if len(text) > 50:  # Avoid empty or too short descriptions\n",
        "                return text\n",
        "    return \"No Wikipedia summary available\"\n",
        "# Base URL for Orphanet diseases\n",
        "base_url = \"https://www.orpha.net/en/disease/detail/\"\n",
        "\n",
        "# Example disease IDs (Replace this with dynamic fetching)\n",
        "disease_ids = range(1, 3000)  # Adjust range as needed\n",
        "\n",
        "disease_data = []\n",
        "\n",
        "for disease_id in disease_ids:\n",
        "    url = f\"{base_url}{disease_id}\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract disease name\n",
        "        title_tag = soup.find(\"div\", class_=\"result-detail\").find(\"h2\")\n",
        "        disease_name = title_tag.text.strip() if title_tag else \"Unknown\"\n",
        "\n",
        "        # Extract description\n",
        "        desc_tag = soup.find(\"div\", class_=\"service-color-box\").find(\"p\")\n",
        "        description = desc_tag.text.strip() if desc_tag else \"No description available\"\n",
        "\n",
        "        # Fetch Wikipedia summary\n",
        "        wikipedia_summary = get_wikipedia_summary(disease_name)\n",
        "\n",
        "        # Save data\n",
        "        disease_data.append({\n",
        "            \"ID\": disease_id,\n",
        "            \"Disease Name\": disease_name,\n",
        "            \"Description\": description,\n",
        "            \"Wikipedia Summary\": wikipedia_summary\n",
        "        })\n",
        "    else:\n",
        "        print(f\"Failed to fetch disease ID {disease_id}\")\n",
        "\n",
        "# Convert to DataFrame and save to CSV\n",
        "df = pd.DataFrame(disease_data)\n",
        "df.to_csv(\"/content/diseases_wikipedia.csv\", index=False)\n",
        "\n",
        "print(\"Scraping complete. Data saved to diseases_wikipedia.csv.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tL-6HpcXj_q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/diseases_wikipedia.csv\")\n",
        "\n",
        "# Combine the text for training\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "df[\"text\"] = df[\"Disease Name\"] + \": \" + df[\"Description\"] + \" \" + df[\"Wikipedia Summary\"]\n",
        "df[\"text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "# Save cleaned text\n",
        "dataset_text = \"\\n\".join(df[\"text\"].tolist())\n",
        "\n",
        "# Save for tokenization\n",
        "with open(\"rare_diseases.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(dataset_text)\n",
        "\n",
        "print(\"Preprocessing Complete. Saved 'rare_diseases.txt'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFmLGYWjHWM5",
        "outputId": "d04b4b37-d8ec-4cd4-fc5b-4c05dbe2b503"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁Long', '▁chain', '▁3-', 'hydroxy', 'acyl', '-', 'CoA', '▁dehydrogenase', '▁deficiency']\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Train a tokenizer\n",
        "spm.SentencePieceTrainer.train(input=\"rare_diseases.txt\", model_prefix=\"tokenizer\", vocab_size=10000)\n",
        "\n",
        "# Load trained tokenizer\n",
        "sp = spm.SentencePieceProcessor(model_file=\"tokenizer.model\")\n",
        "\n",
        "# Test tokenizer\n",
        "print(sp.encode(\"Long chain 3-hydroxyacyl-CoA dehydrogenase deficiency\", out_type=str))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8slPWhkp4e6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import sentencepiece as spm\n",
        "\n",
        "# Train a tokenizer (only needed once)\n",
        "spm.SentencePieceTrainer.train(input=\"rare_diseases.txt\", model_prefix=\"tokenizer\", vocab_size=10000)\n",
        "\n",
        "# Load trained tokenizer\n",
        "sp = spm.SentencePieceProcessor(model_file=\"tokenizer.model\")\n",
        "\n",
        "# Check if it's working\n",
        "print(\"✅ Tokenizer loaded successfully!\")\n",
        "print(\"Vocab size:\", sp.get_piece_size())  # This should now work\n",
        "\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, num_heads=8, num_layers=6):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads)\n",
        "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.transformer(x)\n",
        "        x = self.fc_out(x)\n",
        "        return x\n",
        "\n",
        "# Load tokenizer vocab size\n",
        "vocab_size = sp.get_piece_size()\n",
        "\n",
        "# Initialize model\n",
        "model = TransformerLM(vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1W3infrp6f5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Custom dataset for tokenized text\n",
        "class RareDiseaseDataset(Dataset):\n",
        "    def __init__(self, text, tokenizer, seq_length=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tokens = tokenizer.encode(text)\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_ids = self.tokens[idx : idx + self.seq_length]\n",
        "        target_ids = self.tokens[idx + 1 : idx + self.seq_length + 1]\n",
        "        return torch.tensor(input_ids), torch.tensor(target_ids)\n",
        "\n",
        "# Load dataset\n",
        "dataset = RareDiseaseDataset(dataset_text, sp, seq_length=128)\n",
        "\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define loss function and optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)  # Adjust learning rate if needed\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_text(prompt, model, tokenizer, max_len=100):\n",
        "    model.eval()\n",
        "    tokens = tokenizer.encode(prompt)\n",
        "    input_ids = torch.tensor(tokens).unsqueeze(0).to(device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            if next_token.item() == tokenizer.pad_id():\n",
        "                break\n",
        "\n",
        "            input_ids = torch.cat((input_ids, next_token), dim=1)\n",
        "\n",
        "    return tokenizer.decode(input_ids.squeeze(0).tolist())\n",
        "\n",
        "# Load model and test\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in dataloader:\n",
        "        input_ids, target_ids = batch\n",
        "        input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), target_ids.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Loss: {loss.item()}\")\n",
        "\n",
        "# Save trained model\n",
        "torch.save(model.state_dict(), \"rare_disease_transformer.pth\")\n",
        "print(\"Model trained and saved successfully.\")\n",
        "\n",
        "model.load_state_dict(torch.load(\"rare_disease_transformer.pth\", map_location=device))\n",
        "model.to(device)\n",
        "\n",
        "prompt = \"What is Long chain 3-hydroxyacyl-CoA dehydrogenase deficiency?\"\n",
        "print(generate_text(prompt, model, sp))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+YOOXqQqpeA1tGaig8qz8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}